#computer 
- 程序访问局部性原理
- Cache的基本工作原理
- Cache和主存的映射方式
- Cache中主存块的替换算法
- Cache写策略
---
# Cache的工作原理

存储系统存在的问题：
- 双端口RAM、多模块存储器提高存储器的工作速度，但是优化后速度和cpu差距依然很大
- 综合考虑三个方面：
	- 更高速的存储单元设计
	- 存储器价格和容量的均衡
	- 程序访问的局部性原理
- 最终，通过**Cache-主存**层次来改善存储体系

Cache的工作原理：
- 将一些频繁访问的代码和数据存入一个比主存速度更快的存储单元
- 现在cache一般被集成在cpu内部
- 用SRAM实现，速度快，成本高，集成度更低（从而影响其存储空间不可能会特别大）

那么思考，哪些代码和数据是会被频繁访问到的呢？现实里代码的确是这样吗？
- 利用程序访问的局部性原理

# 程序访问的局部性原理

可以利用指令和数据在内存中的存储来进行理解：

![](../img/Pasted%20image%2020231211092726.png)

- **空间局部性**：最近有可能用到的信息，有可能与现在正在使用的信息在存储空间上是临近的
	- 如上图中的二维数组的存放
- **时间局部性**：最近的未来可能是用到的信息，很可能是现在正在使用的信息
	- 如循环结构的指令代码，每一次迭代都会存在对循环变量i和j的访问
	- 短时间内会被重复访问

所以可以利用局部性原理，将cpu目前访问的地址“周围”的部分数据放到cache中

- 思考：如果把上边代码修改为一列一列访问二维数组呢？
	- 列访问会影响到程序的空间局部性
	- 理论上会比行访问的程序代码更慢

# 性能分析

**命中率H**：cpu欲访问的信息已经在cache中的比率
- 对应的缺失（未命中）率 M=1-H

设 tc 为访问一次cache所需时间，tm 为访问一次主存所需时间（tc < tm）

- cache-主存系统的平均访问时间t：
	- **方案一**：*先在cache中找，找不到再去主存*，本质是求数学期望
		- cache中能找到的概率即为命中率H，需要时间为tc
		- cache中找不到概率为1-H，此时先找cache花费时间tc，再去主存找花费时间tm
		- 故平均访问时间：t = Htc + (1-H)(tc+tm)
	- **方案二**：*同时在cache和主存中找*
		- 如果在cache中找到（命中率 H）停止主存的寻找，则时间为tc
		- 如果cache中找不到，cache先停止（未命中率 1-H），继续在主存中找，则时间为tm
		- 故平均访问时间：t = Htc + (1-H)tm

# 关键问题

## 问题一：如何定义周围？

- 根据局部性原理，将cpu目前访问的地址 *“周围”* 的部分数据放到cache中
- **那么如何定义“周围”？**
	- *将主存的存储空间“分块”*
	- 注意这种分块，*主存块和cache块的大小是相同的*
- **主存和cache之间以块为单位进行数据交换**
	- 注意主存的数据是被复制了一份到cache中
	- *每次被访问的主存块一定会被立即调入cache*

![](../img/Pasted%20image%2020231211094626.png)

---
## 问题二：如何区分cache与主存的数据块的对应关系？

- **cache和主存的映射方式**

## 问题三：cache很小，主存很大，如果cache满了怎么办？

- **替换算法**

## 问题四：cpu修改了cache中的数据副本，如何确保主存中数据母本的一致性？

- **cache写策略**

---

# Cache和主存的映射

- **回想问题**：如何将cache与主存的数据块进行对应？

三种方式：
- **全相联映射**
	- 主存块可以方位cache 的任意位置
- **直接映射**
	- 每个主存块只能放到一个特定位置
	- cache块号 = 主存块号 % cache总块数
- **组相连映射**
	- cache块分若干组，每个主存块放到特定分组中的任意一个位置
	- 组号= 主存块号 % 分组数

![](../img/Pasted%20image%2020231211095329.png)

- **思考问题**：主存到cache的映射完成，但是如何区分*cache中存放的是哪个主存块*？
	- 给每个cache块增加一个 *“标记”*，记录对应的主存块号
	- 计算机内部为二进制表示，则这些标记只有0和1两种，所以为存放数据的cache块其标记会记为0，但这样会变成未标记的块对应了主存0号块，*还需要增加一个有效位*，表示当前是否存放了主存块信息
	- 标记 + 有效位
		- 前者表示对应的主存块号
		- 后者标记是否存放

**全相联**：

![](../img/Pasted%20image%2020231211100330.png)

- 优点：
	- 存储空间利用充分，命中率高
- 缺点：
	- 查找标记慢，可能需要对比所有行的标记

**直接映射（固定位置）**：

- 优点：
	- 任意一个地址，只需要对比一个标记，速度最快
- 缺点：
	- 灵活性差，空间利用率低，命中率低

![](../img/Pasted%20image%2020231211100601.png)
![](../img/Pasted%20image%2020231211100607.png)
![](../img/Pasted%20image%2020231211100636.png)

**组相联映射（特定分组）**

![](../img/Pasted%20image%2020231211100829.png)

- n路组相联映射：每n个cache行为一组

- 优点
	- 自由度高，查找标记速度快
	- 综合效果最好

- 结合每种地址映射方式的地址结构思考
	- **给定一个主存地址，如何拆分地址，并查找cache、访存？**

# cache的替换算法

- **回想问题**：主存很大，cache很小，cache装满后怎么办？

- 根据三种映射讨论cache装满的状态
	- **全相联映射**
		- cache*完全满*了才需要替换，
		- 在*全局选择替换*哪一块
	- 直接映射
		- 对应位置非空，则毫无选择直接替换、
		- *不需要考虑替换算法*
	- **组相联映射**
		- *分组内满了*才需要替换
		- 需要*在分组内*选择替换哪一块

## 以全相联映射为例

四种算法：
- 随机算法 RAND
- 先进先出算法 FIFO
- 近期最少用 LRU
- 最近不经常使用 LFU

### 随机算法 RAND

- cache已满，随机选择一块进行替换

![](../img/Pasted%20image%2020231211101710.png)

- 实现简单
- *完全没考虑局部性原理*，命中率低，实际效果不稳定

### 先进先出算法 FIFO

- cache已满，则替换最先被调入cache的块

![](../img/Pasted%20image%2020231211101834.png)

- 实现简单
- 最开始按照0、1、2、3放入cache，之后轮流替换0、1、2、3
- *仍然没有考虑局部性原理*
	- 最先调入的cache块也有可能是被频繁访问的

- **抖动现象**
	- 频繁的换入换出现象
	- 刚被替换的块很快又被调入

### 近期最少用 LRU

- 注意这里的定义是**多久没有**被访问
- 计数器数值越大，表示越久

- Last Recently Used
- 每个cache块设置一个计数器，用于记录每个cache块*多久没有*被访问
- cache满后，替换计数器最大的

- 一个比较快的做题方法：
	- cache满后，从当前位置往前看
	- 例如从第一个5往左以此为2、1、4，分别对应cache的1号0号和3号
	- 其中2号位是*最近*没有被访问的，则此时替换cache5号块即可
![](../img/Pasted%20image%2020231211102307.png)

- 那么机器如何判断？
- cache块：标记+有效位
- 还需要增加一个*计数器*属性
	- 命中，命中行计数器=0，比其低的计数器+1，其余不变
		- 保证此时命中的行计数器为最小（即最近被访问）
	- 未命中，且有空行，新装入的行计数器=0，其余*非空闲行*全+1
	- 未命中，且无空行，计数值最大的行的信息块被淘汰，新装行的块的计数器=0，其余全+1
- 根据图和具体的判断步骤，自己尝试根据计数器步骤进行模拟一遍

![](../img/Pasted%20image%2020231211103125.png)

- cache块的总数=2^n，则计数器只需要n位
- 且cache装满后所有计数器的值一定不重复

- **基于了局部性原理**
- 实际运行效果很优秀，cache命中率高
- 但被频繁访问到的主存块数量 > cache行数量，则有可能发生抖动
	- 如 1、2、3、4、5、1、2、3、4、5、1、2...

### 最近不经常使用 LFU

- 同样设置计数器，但这里计数器记录的是*被访问过*多少次，cache满后替换计数器最小的

![](../img/Pasted%20image%2020231211103527.png)

- 没有很好遵循局部性原理，实际运行效果不如lru
- 原因：曾经被经常访问的主存块，未来不一定用到
- 计数器容易变得很大
- 根据局部性原理，我们只需要考虑很小的一个时间范围内访问的可能，而不是考虑全局上访问的次数

# cache写策略

- **回想问题**：cache块中为主存数据副本，cpu修改了cache中数据，如何保证主存中数据母本的一致性？
	- 首先要分情况，写命中和写不命中

- *写命中*
	- 全写法
	- 写回法
- *写不命中*
	- 写分配法
	- 非写分配法

- 读操作呢？
	- 读操作不会导致cache和主存中数据的不一致问题

## 写命中

### 写回法

write-back

- cpu对cache写命中时，只修改cache的内容，而不立即写入主存
	- 只有*当此块被换出时才写回*主存

- cache增加一个*脏位*信息，表示被修改过
	- 被替换时，根据脏位判断是否一致
		- 不一致则写回主存
		- 一致则不必写回

- 优点：减少访存次数
- 缺点：存在数据不一致的隐患（因为不是即时更新）

### 全写法

write-through

- cpu对cache写命中，必须把数据*同时写入*cache和主存
	- 一般使用**写缓冲**（write buffer）（SRAM实现FIFO队列，cpu往sram中写操作比直接写入主存更快，专门的电路将写缓冲中同步到主存中）
- 优点：
	- 能保证数据一致性
- 缺点：
	- 访存次数增加，速度变慢
- **写缓冲**
	- cpu写速度很快，写操作不频繁，效果很好
	- 写操作很频繁，可能会因为写缓冲饱和而发生阻塞

## 写不命中

### 写分配法

write-allocate

- cpu对cache写不命中，把主存中的块调入cache，在cache中修改。搭配*写回法*使用

### 非写分配法

non-

- cpu对cache写不命中只写入主存，不调入cache，搭配*全写法*使用
	- 这种分配法：**只有读未命中，才会调入cache**

## 多级cache

![](../img/Pasted%20image%2020231211104843.png)

注意：
- cache之间的策略
- cache与主存之间的策略
